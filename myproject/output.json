[
{"title": "Prompt engineering overview", "content": "Anthropic home page![light logo](https://mintlify.s3-us-\nwest-1.amazonaws.com/anthropic/logo/light.svg)![dark\nlogo](https://mintlify.s3-us-west-1.amazonaws.com/anthropic/logo/dark.svg)\n\nEnglish\n\nSearch...\n\n  * Go to claude.ai\n  * Research\n  * News\n  * Go to claude.ai\n\nSearch\n\nNavigation\n\nPrompt engineering\n\nPrompt engineering overview\n\nUser Guides\n\nAPI Reference\n\nPrompt Library\n\nRelease Notes\n\nBuild with Claude Contest\n\n  * Developer Console\n  * Developer Discord\n  * Support\n\n##### Get started\n\n  * Overview\n\n  * Quickstart\n\n  * Intro to Claude\n\n##### Learn about Claude\n\n  * Use cases\n\n  * Models\n\n  * Security and compliance\n\n##### Build with Claude\n\n  * Define success criteria\n\n  * Develop test cases\n\n  * Prompt engineering\n\n    * Overview\n\n    * Prompt generator\n\n    * Be clear and direct\n\n    * Use examples (multishot prompting)\n\n    * Let Claude think (CoT)\n\n    * Use XML tags\n\n    * Give Claude a role (system prompts)\n\n    * Prefill Claude's response\n\n    * Chain complex prompts\n\n    * Long context tips\n\n  * Text generation\n\n  * Embeddings\n\n  * Google Sheets add-on\n\n  * Vision\n\n  * Tool use (function calling)\n\n##### Test and evaluate\n\n  * Strengthen guardrails\n\n  * Using the Evaluation Tool\n\n##### Resources\n\n  * Glossary\n\n  * System status\n\n  * Claude 3 model card\n\n  * Anthropic Cookbook\n\n  * Anthropic Courses\n\nPrompt engineering\n\n# Prompt engineering overview\n\n##\n\n​\n\nBefore prompt engineering\n\nThis guide assumes that you have:\n\n  1. A clear definition of the success criteria for your use case\n  2. Some ways to empirically test against those criteria\n  3. A first draft prompt you want to improve\n\nIf not, we highly suggest you spend time establishing that first. Check out\nDefine your success criteria and Create strong empirical evaluations for tips\nand guidance.\n\n## Prompt generator\n\nDon’t have a first draft prompt? Try the prompt generator in the Anthropic\nConsole!\n\n* * *\n\n##\n\n​\n\nWhen to prompt engineer\n\nThis guide focuses on success criteria that are controllable through prompt\nengineering. Not every success criteria or failing eval is best solved by\nprompt engineering. For example, latency and cost can be sometimes more easily\nimproved by selecting a different model.\n\nPrompting vs. finetuning\n\nPrompt engineering is far faster than other methods of model behavior control,\nsuch as finetuning, and can often yield leaps in performance in far less time.\nHere are some reasons to consider prompt engineering over finetuning:  \n\n  * **Resource efficiency** : Fine-tuning requires high-end GPUs and large memory, while prompt engineering only needs text input, making it much more resource-friendly.\n  * **Cost-effectiveness** : For cloud-based AI services, fine-tuning incurs significant costs. Prompt engineering uses the base model, which is typically cheaper.\n  * **Maintaining model updates** : When providers update models, fine-tuned versions might need retraining. Prompts usually work across versions without changes.\n  * **Time-saving** : Fine-tuning can take hours or even days. In contrast, prompt engineering provides nearly instantaneous results, allowing for quick problem-solving.\n  * **Minimal data needs** : Fine-tuning needs substantial task-specific, labeled data, which can be scarce or expensive. Prompt engineering works with few-shot or even zero-shot learning.\n  * **Flexibility & rapid iteration**: Quickly try various approaches, tweak prompts, and see immediate results. This rapid experimentation is difficult with fine-tuning.\n  * **Domain adaptation** : Easily adapt models to new domains by providing domain-specific context in prompts, without retraining.\n  * **Comprehension improvements** : Prompt engineering is far more effective than finetuning at helping models better understand and utilize external content such as retrieved documents\n  * **Preserves general knowledge** : Fine-tuning risks catastrophic forgetting, where the model loses general knowledge. Prompt engineering maintains the model’s broad capabilities.\n  * **Transparency** : Prompts are human-readable, showing exactly what information the model receives. This transparency aids in understanding and debugging.\n\n* * *\n\n##\n\n​\n\nHow to prompt engineer\n\nThe prompt engineering pages in this section have been organized from most\nbroadly effective techniques to more specialized techniques. When\ntroubleshooting performance, we suggest you try these techniques in order,\nalthough the actual impact of each technique will depend on our use case.\n\n  1. Prompt generator\n  2. Be clear and direct\n  3. Use examples (multishot)\n  4. Let Claude think (chain of thought)\n  5. Use XML tags\n  6. Give Claude a role (system prompts)\n  7. Prefill Claude’s response\n  8. Chain complex prompts\n  9. Long context tips\n\n* * *\n\n##\n\n​\n\nPrompt engineering tutorial\n\nIf you’re an interactive learner, you can dive into our interactive tutorials\ninstead!\n\n## GitHub prompting tutorial\n\nAn example-filled tutorial that covers the prompt engineering concepts found\nin our docs.\n\n## Google Sheets prompting tutorial\n\nA lighter weight version of our prompt engineering tutorial via an interactive\nspreadsheet.\n\nDevelop test casesPrompt generator\n\nxlinkedin\n\nOn this page\n\n  * Before prompt engineering\n  * When to prompt engineer\n  * How to prompt engineer\n  * Prompt engineering tutorial\n\n", "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview"}
][
{"title": "Prompt engineering overview", "content": [{"section": "Prompt generator", "content": ""}]}
][
{"title": "Prompt engineering overview", "content": []}
][
{"title": "Prompt engineering overview", "content": []}
][
{"title": "Prompt engineering overview", "content": []}
][
{"title": "Prompt engineering overview", "content": []}
][
{"title": "Prompt engineering overview", "content": []}
][
{"title": "Prompt engineering overview", "content": []}
]